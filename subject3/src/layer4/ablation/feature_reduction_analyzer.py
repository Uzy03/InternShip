#!/usr/bin/env python3
"""
ç‰¹å¾´é‡å‰Šæ¸›åˆ†æå™¨

Colabã®çµæœã‹ã‚‰ã€ç‰¹å¾´é‡ã®é‡è¦åº¦ãŒä½ã„ã‚‚ã®ã‚’ç‰¹å®šã—ã€
ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ã®3ã¤ã®ã‚«ãƒ†ã‚´ãƒªã«è©²å½“ã™ã‚‹ç‰¹å¾´é‡ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã€‚
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
from typing import Dict, List, Set, Tuple

# ãƒ‘ã‚¹è¨­å®š
BASE_DIR = Path(__file__).parent.parent.parent.parent
FEATURE_IMPORTANCE_PATH = BASE_DIR / "data" / "processed" / "l4_feature_importance.csv"
COLAB_RESULTS_DIR = BASE_DIR.parent / "colab"

# ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ã®ã‚«ãƒ†ã‚´ãƒªå®šç¾©
ABLATION_CATEGORIES = {
    'foreign': [
        'foreign_change', 'foreign_change_covid', 'foreign_change_post2022',
        'foreign_log', 'foreign_log_covid', 'foreign_log_post2022',
        'foreign_ma3', 'foreign_ma3_covid', 'foreign_ma3_post2022',
        'foreign_pct_change', 'foreign_pct_change_covid', 'foreign_pct_change_post2022',
        'foreign_population', 'foreign_population_covid', 'foreign_population_post2022',
        'foreign_change_rate', 'foreign_change_rate_2022_2023'
    ],
    'spatial': [
        'ring1_exp_commercial_inc_h1', 'ring1_exp_commercial_inc_h2', 'ring1_exp_commercial_inc_h3',
        'ring1_exp_disaster_dec_h1', 'ring1_exp_disaster_dec_h2', 'ring1_exp_disaster_dec_h3',
        'ring1_exp_disaster_inc_h1', 'ring1_exp_disaster_inc_h2', 'ring1_exp_disaster_inc_h3',
        'ring1_exp_employment_inc_h1', 'ring1_exp_employment_inc_h2', 'ring1_exp_employment_inc_h3',
        'ring1_exp_housing_dec_h1', 'ring1_exp_housing_dec_h2', 'ring1_exp_housing_dec_h3',
        'ring1_exp_housing_inc_h1', 'ring1_exp_housing_inc_h2', 'ring1_exp_housing_inc_h3',
        'ring1_exp_public_edu_medical_dec_h1', 'ring1_exp_public_edu_medical_dec_h2'
    ],
    'macro': [
        'macro_delta', 'macro_excl', 'macro_ma3', 'macro_shock'
    ],
    'temporal': [
        'lag_d1', 'lag_d2', 'ma2_delta'
    ],
    'era': [
        'era_covid', 'era_post2009', 'era_post2013', 'era_post2022', 'era_pre2013'
    ],
    'town': [
        'town_ma5', 'town_std5', 'town_trend5'
    ]
}

def load_feature_importance() -> pd.DataFrame:
    """ç‰¹å¾´é‡é‡è¦åº¦ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    df = pd.read_csv(FEATURE_IMPORTANCE_PATH)
    return df.sort_values('importance', ascending=False)

def load_colab_results() -> Dict[str, Dict]:
    """Colabã®çµæœã‚’èª­ã¿è¾¼ã‚€"""
    results = {}
    
    # ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã®çµæœ
    full_model_path = COLAB_RESULTS_DIR / "l4_cv_metrics_feature_engineered.json"
    if full_model_path.exists():
        with open(full_model_path, 'r', encoding='utf-8') as f:
            results['full_model'] = json.load(f)
    
    # ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ã®çµæœ
    ablation_files = [
        "ablation_metrics_no_foreign.json",
        "ablation_metrics_no_spatial.json", 
        "ablation_metrics_no_macro.json"
    ]
    
    for file_name in ablation_files:
        file_path = COLAB_RESULTS_DIR / file_name
        if file_path.exists():
            study_name = file_name.replace("ablation_metrics_", "").replace(".json", "")
            with open(file_path, 'r', encoding='utf-8') as f:
                results[study_name] = json.load(f)
    
    return results

def categorize_features(features: List[str]) -> Dict[str, List[str]]:
    """ç‰¹å¾´é‡ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡"""
    categorized = {category: [] for category in ABLATION_CATEGORIES.keys()}
    uncategorized = []
    
    for feature in features:
        found_category = False
        for category, category_features in ABLATION_CATEGORIES.items():
            if feature in category_features:
                categorized[category].append(feature)
                found_category = True
                break
        
        if not found_category:
            uncategorized.append(feature)
    
    categorized['uncategorized'] = uncategorized
    return categorized

def identify_low_importance_features(df: pd.DataFrame, threshold_percentile: float = 20) -> List[str]:
    """ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾è±¡ç‰¹å¾´é‡ã®ä¸­ã‹ã‚‰é‡è¦åº¦ãŒä½ã„ç‰¹å¾´é‡ã‚’ç‰¹å®š"""
    # ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾è±¡ç‰¹å¾´é‡ã®ã¿ã‚’æŠ½å‡º
    ablation_features = []
    for category_features in ABLATION_CATEGORIES.values():
        ablation_features.extend(category_features)
    
    ablation_df = df[df['feature'].isin(ablation_features)]
    
    if len(ablation_df) == 0:
        return []
    
    # ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾è±¡ç‰¹å¾´é‡ã®ä¸­ã§é‡è¦åº¦ãŒä½ã„ã‚‚ã®ã‚’ç‰¹å®š
    threshold = np.percentile(ablation_df['importance'], threshold_percentile)
    low_importance = ablation_df[ablation_df['importance'] <= threshold]
    
    return low_importance['feature'].tolist()

def analyze_feature_reduction_candidates():
    """ç‰¹å¾´é‡å‰Šæ¸›å€™è£œã‚’åˆ†æ"""
    print("=" * 80)
    print("ç‰¹å¾´é‡å‰Šæ¸›åˆ†æ")
    print("=" * 80)
    
    # ç‰¹å¾´é‡é‡è¦åº¦ã‚’èª­ã¿è¾¼ã¿
    print("\nğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    df_importance = load_feature_importance()
    print(f"ç·ç‰¹å¾´é‡æ•°: {len(df_importance)}")
    
    # Colabã®çµæœã‚’èª­ã¿è¾¼ã¿
    print("\nğŸ“Š Colabã®çµæœã‚’èª­ã¿è¾¼ã¿ä¸­...")
    colab_results = load_colab_results()
    
    if 'full_model' in colab_results:
        full_model_metrics = colab_results['full_model']['aggregate']
        print(f"ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:")
        print(f"  MAE: {full_model_metrics['MAE']:.4f}")
        print(f"  RMSE: {full_model_metrics['RMSE']:.4f}")
        print(f"  MAPE: {full_model_metrics['MAPE']:.4f}")
        print(f"  RÂ²: {full_model_metrics['R2']:.4f}")
    
    # ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ã®æ€§èƒ½ã‚’è¡¨ç¤º
    print(f"\nğŸ“Š ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ã®æ€§èƒ½:")
    for study_name, study_data in colab_results.items():
        if study_name != 'full_model' and 'aggregate_metrics' in study_data:
            metrics = study_data['aggregate_metrics']
            print(f"{study_name}:")
            print(f"  MAE: {metrics['MAE']:.4f}")
            print(f"  RMSE: {metrics['RMSE']:.4f}")
            print(f"  MAPE: {metrics['MAPE']:.4f}")
            print(f"  RÂ²: {metrics['R2']:.4f}")
    
    # ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾è±¡ç‰¹å¾´é‡ã®ä¸­ã§é‡è¦åº¦ãŒä½ã„ç‰¹å¾´é‡ã‚’ç‰¹å®š
    print(f"\nğŸ” ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾è±¡ç‰¹å¾´é‡ã®ä¸­ã§é‡è¦åº¦ãŒä½ã„ç‰¹å¾´é‡ã‚’åˆ†æä¸­...")
    low_importance_features = identify_low_importance_features(df_importance, threshold_percentile=20)
    print(f"ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾è±¡ç‰¹å¾´é‡ã®ä¸­ã§é‡è¦åº¦ä¸‹ä½20%ã®ç‰¹å¾´é‡æ•°: {len(low_importance_features)}")
    
    # ç‰¹å¾´é‡ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡
    print(f"\nğŸ“‹ ç‰¹å¾´é‡ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ä¸­...")
    categorized = categorize_features(df_importance['feature'].tolist())
    
    for category, features in categorized.items():
        if category != 'uncategorized':
            print(f"{category}: {len(features)}å€‹")
    
    # å‰Šæ¸›å€™è£œã‚’ç‰¹å®š
    print(f"\nğŸ¯ å‰Šæ¸›å€™è£œã®åˆ†æ:")
    
    reduction_candidates = {
        'low_importance_by_category': {},
        'zero_importance': [],
        'recommended_reduction': {}
    }
    
    # ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾è±¡ç‰¹å¾´é‡ã®ä¸­ã§é‡è¦åº¦0ã®ç‰¹å¾´é‡ã‚’å‰Šé™¤å¯¾è±¡ã«
    ablation_features = []
    for category_features in ABLATION_CATEGORIES.values():
        ablation_features.extend(category_features)
    
    ablation_df = df_importance[df_importance['feature'].isin(ablation_features)]
    zero_importance = ablation_df[ablation_df['importance'] == 0.0]['feature'].tolist()
    reduction_candidates['zero_importance'] = zero_importance
    
    print(f"ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾è±¡ç‰¹å¾´é‡ã®ä¸­ã§é‡è¦åº¦0ã®ç‰¹å¾´é‡ï¼ˆå‰Šé™¤å¯¾è±¡ï¼‰: {len(zero_importance)}å€‹")
    for feature in zero_importance:
        print(f"  - {feature}")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ä½é‡è¦åº¦ç‰¹å¾´é‡
    for category, category_features in ABLATION_CATEGORIES.items():
        category_low_importance = []
        for feature in low_importance_features:
            if feature in category_features:
                category_low_importance.append(feature)
        
        if category_low_importance:
            reduction_candidates['low_importance_by_category'][category] = category_low_importance
            print(f"\n{category}ã‚«ãƒ†ã‚´ãƒªã®ä½é‡è¦åº¦ç‰¹å¾´é‡ ({len(category_low_importance)}å€‹):")
            for feature in category_low_importance:
                importance = df_importance[df_importance['feature'] == feature]['importance'].iloc[0]
                print(f"  - {feature}: {importance:.2f}")
    
    # æ¨å¥¨å‰Šæ¸›æˆ¦ç•¥
    print(f"\nğŸ’¡ æ¨å¥¨å‰Šæ¸›æˆ¦ç•¥:")
    
    # 1. ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾è±¡ç‰¹å¾´é‡ã®ä¸­ã§é‡è¦åº¦0ã®ç‰¹å¾´é‡ã‚’å‰Šé™¤
    if zero_importance:
        print(f"1. ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾è±¡ç‰¹å¾´é‡ã®ä¸­ã§é‡è¦åº¦0ã®ç‰¹å¾´é‡ã‚’å‰Šé™¤: {len(zero_importance)}å€‹")
        for feature in zero_importance:
            print(f"   - {feature}")
    
    # 2. å„ã‚«ãƒ†ã‚´ãƒªã§é‡è¦åº¦ãŒä½ã„ç‰¹å¾´é‡ã‚’å‰Šé™¤
    print(f"\n2. ã‚«ãƒ†ã‚´ãƒªåˆ¥ä½é‡è¦åº¦ç‰¹å¾´é‡ã®å‰Šé™¤:")
    for category, low_features in reduction_candidates['low_importance_by_category'].items():
        if low_features:
            print(f"   {category}: {len(low_features)}å€‹")
            for feature in low_features[:3]:  # ä¸Šä½3å€‹ã®ã¿è¡¨ç¤º
                importance = df_importance[df_importance['feature'] == feature]['importance'].iloc[0]
                print(f"     - {feature}: {importance:.2f}")
            if len(low_features) > 3:
                print(f"     ... ä»–{len(low_features)-3}å€‹")
    
    # å‰Šæ¸›å¾Œã®ç‰¹å¾´é‡æ•°ã‚’è¨ˆç®—
    features_to_remove = set(zero_importance)
    for low_features in reduction_candidates['low_importance_by_category'].values():
        features_to_remove.update(low_features)
    
    remaining_features = len(df_importance) - len(features_to_remove)
    print(f"\nğŸ“ˆ å‰Šæ¸›åŠ¹æœ:")
    print(f"ç¾åœ¨ã®ç‰¹å¾´é‡æ•°: {len(df_importance)}")
    print(f"å‰Šé™¤ã™ã‚‹ç‰¹å¾´é‡æ•°: {len(features_to_remove)}")
    print(f"å‰Šæ¸›å¾Œã®ç‰¹å¾´é‡æ•°: {remaining_features}")
    print(f"å‰Šæ¸›ç‡: {len(features_to_remove)/len(df_importance)*100:.1f}%")
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_data = {
        'analysis_summary': {
            'total_features': len(df_importance),
            'features_to_remove': len(features_to_remove),
            'remaining_features': remaining_features,
            'reduction_rate': len(features_to_remove)/len(df_importance)*100
        },
        'zero_importance_features': zero_importance,
        'low_importance_by_category': reduction_candidates['low_importance_by_category'],
        'all_features_to_remove': list(features_to_remove),
        'remaining_features_list': [f for f in df_importance['feature'].tolist() if f not in features_to_remove],
        'note': 'ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ã§å¯¾è±¡ã¨ãªã£ãŸç‰¹å¾´é‡ã®ä¸­ã‹ã‚‰é‡è¦åº¦ãŒä½ã„ã‚‚ã®ã‚’å‰Šé™¤å¯¾è±¡ã¨ã—ãŸ'
    }
    
    output_path = BASE_DIR / "src" / "layer4" / "ablation" / "feature_reduction_analysis.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ åˆ†æçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    
    return output_data

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    try:
        result = analyze_feature_reduction_candidates()
        print(f"\nğŸ‰ ç‰¹å¾´é‡å‰Šæ¸›åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
