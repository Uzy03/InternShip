#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰‹å‹•èª¿æŸ»ç”¨CSVãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå®Œç’§ç‰ˆï¼‰
æ¿€å¢—ãƒ»å¤§å¹…å¢—åŠ ãƒ»å¤§å¹…æ¸›å°‘ã®ãƒ‡ãƒ¼ã‚¿ã«åŸå› ã‚³ãƒ©ãƒ ã‚’è¿½åŠ 
é‡è¤‡ãƒ»ç·æ•°è¡Œã‚’æœ€åˆã‹ã‚‰é™¤å¤–
"""

import pandas as pd
import numpy as np
import os

def create_manual_investigation_csv():
    """æ‰‹å‹•èª¿æŸ»ç”¨ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆï¼ˆé‡è¤‡ãƒ»ç·æ•°è¡Œã‚’é™¤å¤–ï¼‰"""
    
    # 1. åˆ†é¡æ¸ˆã¿äººå£å¤‰åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    print("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ä¸­...")
    #script_dir = os.path.dirname(os.path.abspath(__file__))
    data_path = "subject2/processed_data/classified_population_changes.csv"
    
    try:
        df = pd.read_csv(data_path, encoding='utf-8')
        print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {df.shape}")
    except FileNotFoundError:
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_path}")
        return
    
    # 2. ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    print("ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...")
    
    # ã€Œç·æ•°ã€è¡Œã‚’é™¤å¤–
    df_cleaned = df[df['ç”ºä¸å'] != 'ç·æ•°']
    print(f"ã€Œç·æ•°ã€è¡Œé™¤å¤–å¾Œ: {len(df_cleaned)}ä»¶")
    
    # 3. èª¿æŸ»å¯¾è±¡ã®åˆ†é¡ã‚’æŠ½å‡º
    target_categories = ['æ¿€å¢—', 'å¤§å¹…å¢—åŠ ', 'å¤§å¹…æ¸›å°‘', 'æ¿€æ¸›']
    target_data = df_cleaned[df_cleaned['å¤‰åŒ–ã‚¿ã‚¤ãƒ—'].isin(target_categories)].copy()
    
    print(f"èª¿æŸ»å¯¾è±¡ä»¶æ•°: {len(target_data)}ä»¶")
    
    # 4. é‡è¤‡é™¤å»ï¼ˆç”ºä¸åãƒ»å¹´åº¦ã®çµ„ã¿åˆã‚ã›ã§ï¼‰
    target_data = target_data.drop_duplicates(subset=['ç”ºä¸å', 'å¹´åº¦'], keep='first')
    print(f"é‡è¤‡é™¤å»å¾Œ: {len(target_data)}ä»¶")
    
    # 5. æ—¢çŸ¥ã®åŸå› ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
    known_causes = {
        'æ…¶å¾³å €ç”º_2013': 'åˆ†è­²ãƒãƒ³ã‚·ãƒ§ãƒ³ã€Œã‚¨ã‚¤ãƒ«ãƒãƒ³ã‚·ãƒ§ãƒ³æ…¶å¾³ã‚¤ã‚¯ã‚·ã‚ªã€ç«£å·¥',
        'é­šå±‹ç”ºï¼‘ä¸ç›®_2020': 'ã€Œï¼ˆä»®ç§°ï¼‰é­šå±‹ç”ºãƒ»ç´ºå±‹ç”ºãƒãƒ³ã‚·ãƒ§ãƒ³æ–°ç¯‰å·¥äº‹ã€å®Œæˆ',
        'ç§‹æ´¥æ–°ç”º_2019': 'ã€Œç§‹æ´¥æ–°ç”ºãƒãƒ³ã‚·ãƒ§ãƒ³ æ–°ç¯‰å·¥äº‹ã€å®Œæˆ',
        'é›å†¶å±‹ç”º_2014': 'å¤§å‹åˆ†è­²ã€Œã‚°ãƒ©ãƒ³ãƒ‰ã‚ªãƒ¼ã‚¯å”äººç”ºé€šã‚Šã€ç«£å·¥',
        'æ²³åŸç”º_2006': 'ã€Œã‚¨ãƒãƒ¼ãƒ©ã‚¤ãƒ•ç†Šæœ¬ä¸­å¤®ã€ç«£å·¥',
        'åƒè‘‰åŸç”º_2013': 'ã€Œã‚¢ãƒ³ãƒ”ãƒ¼ãƒ«ç†Šæœ¬åŸã€å®Œæˆ',
        'æœ¬å±±ï¼“ä¸ç›®_2009': 'ç†Šæœ¬é§…æ±å´ã®å¤§è¦æ¨¡ãƒãƒ³ã‚·ãƒ§ãƒ³ç¾¤å®Œæˆ',
        'æ˜¥æ—¥ï¼“ä¸ç›®_2013': 'ç†Šæœ¬é§…å‘¨è¾ºå†é–‹ç™ºã®é€²è¡Œ',
        'è¿‘è¦‹ï¼‘ä¸ç›®_2001': 'é›†åˆä½å®…ã€Œãƒ—ãƒ©ãƒ¼ãƒã‚¦ã‚¨ã‚¹ãƒˆã€ãªã©ç«£å·¥'
    }
    
    # 6. åŸå› ã‚³ãƒ©ãƒ ã‚’è¿½åŠ 
    target_data['åŸå› '] = np.nan
    
    # 7. æ—¢çŸ¥ã®åŸå› ã‚’è¨­å®š
    for idx, row in target_data.iterrows():
        key = f"{row['ç”ºä¸å']}_{row['å¹´åº¦']}"
        if key in known_causes:
            target_data.at[idx, 'åŸå› '] = known_causes[key]
            print(f"âœ“ æ—¢çŸ¥ã®åŸå› è¨­å®š: {row['ç”ºä¸å']} ({row['å¹´åº¦']}å¹´)")
    
    # 8. åŸå› ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ä»¶æ•°ã‚’ç¢ºèª
    known_count = target_data['åŸå› '].notna().sum()
    unknown_count = target_data['åŸå› '].isna().sum()
    
    print(f"\nåŸå› è¨­å®šçŠ¶æ³:")
    print(f"  æ—¢çŸ¥ã®åŸå› : {known_count}ä»¶")
    print(f"  åŸå› ä¸æ˜: {unknown_count}ä»¶")
    
    # 9. æ‰‹å‹•èª¿æŸ»ç”¨CSVã¨ã—ã¦ä¿å­˜
    output_path = "subject2-2_v2/manual_investigation_targets.csv"
    target_data.to_csv(output_path, index=False, encoding='utf-8')
    
    print(f"\næ‰‹å‹•èª¿æŸ»ç”¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    
    # 10. ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(f"\n=== æ‰‹å‹•èª¿æŸ»å¯¾è±¡ã‚µãƒãƒªãƒ¼ ===")
    print(f"ç·ä»¶æ•°: {len(target_data)}ä»¶")
    
    # å¤‰åŒ–ã‚¿ã‚¤ãƒ—åˆ¥ã®ä»¶æ•°
    type_counts = target_data['å¤‰åŒ–ã‚¿ã‚¤ãƒ—'].value_counts()
    print(f"\nå¤‰åŒ–ã‚¿ã‚¤ãƒ—åˆ¥ä»¶æ•°:")
    for change_type, count in type_counts.items():
        print(f"  {change_type}: {count}ä»¶")
    
    # åŸå› ä¸æ˜ã®ä»¶æ•°ï¼ˆæ‰‹å‹•èª¿æŸ»ãŒå¿…è¦ï¼‰
    print(f"\næ‰‹å‹•èª¿æŸ»ãŒå¿…è¦ãªä»¶æ•°: {unknown_count}ä»¶")
    
    # åŸå› ä¸æ˜ã®è©³ç´°ï¼ˆä¸Šä½10ä»¶ï¼‰
    unknown_data = target_data[target_data['åŸå› '].isna()].head(10)
    print(f"\nåŸå› ä¸æ˜ã®ä¸Šä½10ä»¶:")
    for idx, row in unknown_data.iterrows():
        print(f"  {row['ç”ºä¸å']} ({row['å¹´åº¦']}å¹´) - {row['å¤‰åŒ–ã‚¿ã‚¤ãƒ—']} - å¤‰åŒ–ç‡: {row['å¤‰åŒ–ç‡(%)']:.1f}%")
    
    # 11. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
    print(f"\n=== ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ ===")
    
    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    duplicates = target_data.duplicated(subset=['ç”ºä¸å', 'å¹´åº¦'])
    duplicate_count = duplicates.sum()
    print(f"é‡è¤‡ãƒã‚§ãƒƒã‚¯: {'âœ“ ãªã—' if duplicate_count == 0 else f'âš ï¸ {duplicate_count}ä»¶'}")
    
    # ã€Œç·æ•°ã€è¡Œãƒã‚§ãƒƒã‚¯
    total_rows = target_data[target_data['ç”ºä¸å'] == 'ç·æ•°']
    total_count = len(total_rows)
    print(f"ã€Œç·æ•°ã€è¡Œãƒã‚§ãƒƒã‚¯: {'âœ“ ãªã—' if total_count == 0 else f'âš ï¸ {total_count}ä»¶'}")
    
    # 12. å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    print(f"\nğŸ‰ å®Œç’§ãªCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print(f"   ã“ã‚Œã§å€‹åˆ¥ã®ä¿®æ­£ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä¸è¦ã§ã™")
    
    return target_data

if __name__ == "__main__":
    create_manual_investigation_csv()
